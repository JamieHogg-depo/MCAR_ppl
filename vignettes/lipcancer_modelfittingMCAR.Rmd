---
title: "Efficient Multivariate Conditional Autoregressive (MCAR) prior in Stan"
author:
  name: "By James Hogg - 2023"
output: 
  html_document:
    toc: true
bibliography: bib.bib
csl: research-in-number-theory.csl
editor_options: 
  chunk_output_type: console
---

\newcommand{\lb}[1]{\left( #1 \right)}
\newcommand{\jdist}[2]{\text{#1}\left( #2 \right)}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## Load packages ## ------------------------------------------------------------

library(tidyverse)
library(sf)
library(spdep)
library(mvtnorm)
library(rstan)
library(patchwork)
library(CARBayes)     # A package for fitting CAR models 
library(CARBayesdata) # A package with the Scottish lip cancer data
library(igraph)       # check that weight matrices are entirely connected
rm(list = ls())

#' @param W binary contiguity matrix (must be complete)
#' @param type (defaults to 'pcar') but also takes 'lcar'
prep4MCAR <- function(W, type = "pcar"){
  if(type == "pcar"){
    
    # create sparse matrices
    W <- Matrix::Matrix(W, sparse = TRUE)
    I <- Matrix::Diagonal(nrow(W))
    D <- Matrix::Diagonal( x = Matrix::rowSums(W) )
    C <- solve(D) %*% W
    
    # Eigenvalues of C
    C_eigenvalues <- eigen(C)$values
    
    # get the CRS representation of C
    crs <- rstan::extract_sparse_parts(I+C)
    nC_w <- length(crs$w)
    
    # prepare output list
    return(
      list(C_eigenvalues = C_eigenvalues, 
           nC_w = nC_w,
           C_w = crs$w,
           C_v = crs$v,
           C_u = crs$u,
           D_id_C_w = which(crs$w == 1),
           offD_id_C_w = which(crs$w != 1))
  )
  
  }
  if(type == "lcar"){
    
    # create sparse matrices
    W <- Matrix::Matrix(W, sparse = TRUE)
    D <- Matrix::Diagonal( x = Matrix::rowSums(W) )
    I <- Matrix::Diagonal(nrow(W))
    C <- I - D + W
    
    # get indices for diagonals
    jt <- rstan::extract_sparse_parts(W + 5*I)
    D_id_C_w <- which(jt$w == 5)
    offD_id_C_w <- which(jt$w == 1)
    
    # Eigenvalues of C
    C_eigenvalues <- eigen(C)$values
    
    # get the CRS representation of C
      # add an extra 1 to all diagonals to ensure they
      # captured by `extract_sparse_parts
    crs <- rstan::extract_sparse_parts(C + I)
    nC_w <- length(crs$w)
    
    # Remove 1 from the diagonals 
    crs$w[D_id_C_w] <- crs$w[D_id_C_w] - 1
    
    # prepare output list
    return(
      list(C = as.matrix(C),
           C_eigenvalues = C_eigenvalues, 
           nC_w = nC_w,
           C_w = crs$w,
           C_v = crs$v,
           C_u = crs$u,
           D_id_C_w = D_id_C_w,
           offD_id_C_w = offD_id_C_w)
  )
    
  }
  
}
```

# Introduction

Multivariate conditional autoregressive (MCAR) priors are one way of simultaneously accommodating spatial structure and dependence between several factors [@RN610]. 

# Simulate data

Like many vignettes on spatial modelling we use the well known scottish lip cancer dataset to compare our Stan results to those from CARBayes. 

```{r eval=TRUE}
# Load data
data(lipdata)
data(lipdbf)
data(lipshp)
lipdbf$dbf <- lipdbf$dbf[ ,c(2,1)]
data <- st_as_sf(combine.data.shapefile(data=lipdata, shp=lipshp, dbf=lipdbf))

# create weight matrices
W <- nb2mat(poly2nb(data), zero.policy = FALSE, style = "B")

# Map the data
data %>% 
  ggplot()+
  theme_void()+
  geom_sf()
```

## Using CARBayes

First, we'll fit the Poisson Leroux model using CARBayes. 

```{r}

# fit model
m_s <- Sys.time()
cb_fit <- S.CARleroux(observed ~ offset(log(expected)), data = data, family = "poisson",
                      burnin = 8000, n.sample = 12000, thin = 10, W = W)
(rtmins_cb <- as.numeric(Sys.time() - m_s, units = "mins"))

# get summary measures
cb_summ <- bind_rows(list(as.matrix(cb_fit$samples$beta) %>% 
    posterior::summarise_draws() %>% 
      mutate(variable = "beta"),
as.matrix(cb_fit$samples$rho) %>% 
    posterior::summarise_draws() %>% 
      mutate(variable = "rho"),
as.matrix(cb_fit$samples$tau2) %>% 
    posterior::summarise_draws() %>% 
      mutate(variable = "tau2")))
```

## Using Stan

Compile the models.

```{r}
# compile model
comp <- stan_model(file = "vignettes/stan/PoissonLeroux.stan")

# data list
C_for_stan <- prep4MCAR(W, type = "lcar")
d_stan <- list(M = nrow(data), y = data$observed,
               E = data$expected,
               precision = as.matrix(1), inverse_precision = as.matrix(1))
d_stan <- c(d_stan, C_for_stan)

# fit the Poisson Leroux model
m_s <- Sys.time()
stan_fit <- sampling(object = comp, 
                  data = d_stan, 
                  pars = "theta_mat", include = FALSE,
                  chains = 4, iter = 3000, warmup = 2000,
                  cores = 2) #, refresh = 0)
(rtmins_stan <- as.numeric(Sys.time() - m_s, units = "mins"))
  # Summarise
    # get draws
    its <- rstan::extract(stan_fit)
    # get summary
    summ <- summary(stan_fit)$summary %>% 
      as.data.frame() %>% 
      mutate(n_eff_per_min = n_eff/rtmins_stan) %>% 
      rownames_to_column("parameter")

# takes 20 mins for M=1000, K = 3 - without dot product
# under 2 minute for M=1000, K = 3 - with dot product
# under 4 minute for M=2000, K = 3 - with dot product
# 7 mins to do M=2000, k = 5
```

Given we generated these data, we can assess how well our model estimates the known parameters. We'll start with the spatial autocorrelation parameter, $\rho$. 

```{r echo = FALSE}
# plot the density
data.frame(efficient = its_e$rho,
           naive = its_n$rho) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value, col = name))+theme_bw()+
  geom_density()+
  geom_vline(xintercept = rho)

# compare the sampling efficiency
cbind(summ_e %>% 
        filter(str_detect(parameter, "rho")) %>% 
        dplyr::select(parameter, n_eff_per_min) %>% 
        rename(efficient = n_eff_per_min),
      summ_n %>% 
        filter(str_detect(parameter, "rho")) %>% 
        dplyr::select(n_eff_per_min) %>% 
        rename(naive = n_eff_per_min))
```

As we'd expect our efficient implementation gives a very similar marginal posterior for $\rho$, but with 35 times the sampling efficiency . Next, we'll examine the marginal posteriors for the standard deviations. 

```{r echo = FALSE}
# plot the density
ll <- list()
for(i in 1:5){
  ll[[i]] <- data.frame(efficient = its_e$sd_R[,i],
           naive = its_n$sd_R[,i]) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value, fill = name))+theme_bw()+
  geom_density(alpha = 0.3)+
  geom_vline(xintercept = Sigma_R[i,i])+
  labs(title = paste0("sd_R[", i, "]"),
       fill = "")
}
ll[[1]]+ll[[2]]+ll[[3]]+ll[[4]]+ll[[5]]

# compare the sampling efficiency
cbind(summ_e %>% 
        filter(str_detect(parameter, "sd_R")) %>% 
        dplyr::select(parameter, n_eff_per_min) %>% 
        rename(efficient = n_eff_per_min),
      summ_n %>% 
        filter(str_detect(parameter, "sd_R")) %>% 
        dplyr::select(n_eff_per_min) %>% 
        rename(naive = n_eff_per_min))
```

Similar to before, the naive and efficient implementations provide very similar marginal posterior densities. This time, our approach provides 40 times better sampling efficiency. 

Finally, we'll investigate the posterior densities of the parameters in the first row of the correlation matrix.


```{r}
# plot the density
ll <- list()
for(i in 2:5){
  ll[[i]] <- data.frame(efficient = its_e$Cor_R[,1,i],
           naive = its_n$Cor_R[,1,i]) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value, fill = name))+theme_bw()+
  geom_density(alpha = 0.3)+
  geom_vline(xintercept = Cor_R[1,i])+
  labs(title = paste0("Cor_R[1,", i, "]"),
       fill = "")
}
ll[[2]]+ll[[3]]+ll[[4]]+ll[[5]]

# compare the sampling efficiency
cbind(summ_e %>% 
        filter(str_detect(parameter, "Cor_R\\[1,")) %>% 
        dplyr::select(parameter, n_eff_per_min) %>% 
        rename(efficient = n_eff_per_min),
      summ_n %>% 
        filter(str_detect(parameter, "Cor_R\\[1,")) %>% 
        dplyr::select(n_eff_per_min) %>% 
        rename(naive = n_eff_per_min))
```

## Stress test

The example above was on a relatively small dataset as we wanted to fit the naive approach. In this section, we'll generate a wide range of much larger datasets in order to assess the scalability of our efficient implementation. In addition to the size of the data, we consider a range `c(0,0.2,0.5,0.8,0.98)` for `rho`. 

```{r echo=FALSE, eval=FALSE}
grid <- expand.grid(M = c(200, 500, 1000, 2000),
                    rho = c(0, 0.2, 0.5, 0.8, 0.98))

for(i in 1:nrow(grid)){
  
# number of areas
M <- grid$M[i]
# number of factors
K <- 5
# set the spatial smoothing parameter
rho <- grid$rho[i] # common to all factors

# Get weight matrix for fully connected grid
spat_obj <- getGRID(M)
W <- spat_obj$W
M <- nrow(W)

# dimension of multivariate normal
n <- M*K

## Spatial precision matrix ----
I <- diag(x=1, nrow = M)
D <- diag(rowSums(W))
D_W <- rowSums(W)
C <- diag(1/rowSums(W)) %*% W
Omega_S <- D %*% (I - rho * C)
# Use our user-made function to get the required
# sparse elements from C
C_for_stan <- prep4MCAR(W) # creates a list

## Between factors correlation matrix ----
set.seed(26)
Cor_R <- randcorr(K)
sd_R <- c(1,2,0.2,3,5)
# covariance matrix
Sigma_R <- diag(sd_R) %*% Cor_R %*% diag(sd_R)
# precision matrix
Omega_R <- solve(Sigma_R)

## Derive full precision matrix ----
Omega_A <- kronecker(Omega_S, Omega_R)

## Derive full covariance matrix ----
m_s <- Sys.time(); Sigma_A <- solve(Omega_A); Sys.time()-m_s # takes 15 mins

# Generate multivariate outcome ----
set.seed(80)
# vector version
m_s <- Sys.time(); theta <- MASS::mvrnorm(1, rep(0,n), Sigma_A); Sys.time()-m_s # takes 35 mins
theta_v <- t(theta)

# matrix version - as input needed for input Stan
theta_mat <- matrix(theta_v, nrow = M, ncol = K, byrow = T)

# save data
saveRDS(list(spat_obj = spat_obj,
             C_for_stan = C_for_stan,
             D_W = D_W,
             theta_mat = theta_mat, 
             Sigma_R = Sigma_R, 
             Cor_R = Cor_R), 
        paste0("data/data_m", M, "_k", K, "_rho", str_remove(rho, "\\."), ".rds"))

message("Finished ", i)
}
```

Collect the two sets of data into their respective lists. 

```{r, echo=FALSE, eval=FALSE}
# create grid
grid <- expand.grid(M = c(200, 500, 1000, 2000),
                    rho = c(0, 0.2, 0.5, 0.8, 0.98)) %>% 
  mutate(rtmins = as.numeric(NA),
         rho_median = as.numeric(NA),
         rho_se = as.numeric(NA),
         rho_eff = as.numeric(NA))

# fun for loop
for(i in 1:nrow(grid)){
  
        # number of areas
        M <- grid$M[i]
        # number of factors
        K <- 5
        # set the spatial smoothing parameter
        rho <- grid$rho[i] # common to all factors
        
        # Get weight matrix for fully connected grid
        spat_obj <- getGRID(M)
        W <- spat_obj$W
        M <- nrow(W)
        
        # dimension of multivariate normal
        n <- M*K
        
        ## Spatial precision matrix ----
        I <- diag(x=1, nrow = M)
        D <- diag(rowSums(W))
        D_W <- rowSums(W)
        C <- diag(1/rowSums(W)) %*% W
        Omega_S <- D %*% (I - rho * C)
        # Use our user-made function to get the required
        # sparse elements from C
        C_for_stan <- prep4MCAR(W) # creates a list
  
  # load data
  out <- readRDS(paste0("data/data_m", M, "_k", K, "_rho", str_remove(rho, "\\."), ".rds"))
  list2env(out, globalenv())
  
  # data list
  data_e <- list(M = M, K = K,
             theta_mat = theta_mat, D_W = D_W,
             rho_set = -1)
  data_e <- c(data_e, C_for_stan)
  
  # Run model
  m_s <- Sys.time()
  fit_e <- sampling(object = comp_e, 
                  data = data_e, 
                  chains = 2, iter = 2000, warmup = 1000,
                  cores = 2, refresh = 0)
  grid$rtmins[i] <- as.numeric(Sys.time() - m_s, units = "mins")
  summ_e <- summary(fit_e)$summary %>% 
      as.data.frame() %>% 
      mutate(n_eff_per_min = n_eff/grid$rtmins[i]) %>% 
      rownames_to_column("parameter")
  
  # summarize rho
  its_e <- rstan::extract(fit_e)
  grid$rho_median[i] <- median(its_e$rho)
  grid$rho_se[i] <- sd(its_e$rho)
  grid$rho_eff[i] <- (summ_e %>% filter(str_detect(parameter, "rho")))$n_eff_per_min
  
  message("Finished ", i)
}

saveRDS(grid, paste0("data/completed_grid.rds"))
```

Fitting our efficient implementation to all combinations of `M` and `rho`, we can visualize the accuracy and sampling efficiency. To enable a fair comparison, we'll compare the sampling efficiency for `rho`. 

```{r}
# run time
(grid %>% 
  ggplot(aes(y = rtmins, x = M))+
  theme_bw()+
  geom_point()+
  geom_smooth(method = "lm")+
  labs(y = "Run time (mins)",
       x = "Number of areas"))+
(grid %>% 
  ggplot(aes(y = rtmins, x = rho))+
  theme_bw()+
  geom_point()+
  labs(y = "Run time (mins)",
       x = "rho"))

# efficiency
(grid %>% 
  ggplot(aes(y = rho_eff, x = M))+
  theme_bw()+
  geom_point()+
  geom_smooth()+
  labs(y = "ESS per minute for rho",
       x = "Number of areas"))+
(grid %>% 
  ggplot(aes(y = rho_eff, x = rho))+
  theme_bw()+
  geom_point()+
  labs(y = "ESS per minute for rho",
       x = "rho"))

# posterior median
grid %>% 
  ggplot(aes(y = rho_median, x = rho))+
  theme_bw()+
  geom_point()+
  geom_abline()+
  labs(y = "Posterior median of rho",
       x = "Fixed rho")
```


# References

<div id="refs"></div>
